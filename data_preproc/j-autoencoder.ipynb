{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-encoding images in latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### base modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 17:12:01.042900: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-06 17:12:01.045480: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-06 17:12:01.137856: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-06 17:12:01.139621: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 17:12:02.340379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = \"raw_data/photos/movies_100\" # contains atm 4_659 images\n",
    "IMAGE_WIDTH = 350\n",
    "IMAGE_HEIGHT = 525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def build_unsupervised_dataset(images_path=IMAGES_PATH,\n",
    "                               width=IMAGE_WIDTH, height=IMAGE_HEIGHT,\n",
    "                               batch_size=32):\n",
    "\n",
    "    return image_dataset_from_directory(\n",
    "        images_path,\n",
    "        labels=None,\n",
    "        label_mode=None,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(height, width),\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "    ).map(\n",
    "        lambda x: x / 255.0\n",
    "    ).map(\n",
    "        lambda x: (x, x) # X and Y are the same (for the auto-encoder)\n",
    "    )\n",
    "\n",
    "dataset = build_unsupervised_dataset(batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([8, 525, 350, 3]), TensorShape([8, 525, 350, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = iter(dataset)\n",
    "e = next(i)\n",
    "e[0].shape, e[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_encoder(embedding_dim=30):\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(embedding_dim, activation=\"relu\"),\n",
    "    ], name=\"encoder\")\n",
    "    \n",
    "def basic_decoder(embedding_dim=30):\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=(embedding_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(IMAGE_HEIGHT * IMAGE_WIDTH * 3, activation=\"sigmoid\"),\n",
    "        layers.Reshape((IMAGE_HEIGHT, IMAGE_WIDTH, 3)),\n",
    "    ], name=\"decoder\")\n",
    "\n",
    "\n",
    "def basic_autoencoder(embedding_dim=30):\n",
    "    return models.Sequential([\n",
    "        basic_encoder(embedding_dim),\n",
    "        basic_decoder(embedding_dim),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = basic_autoencoder(3)\n",
    "encoder = autoencoder.get_layer('encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 18s 1s/step - loss: 0.1156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fbbe433d690>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get encodings of batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 277ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = dataset.take(1)\n",
    "\n",
    "encodings = encoder.predict(batch)\n",
    "encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get images after processing by the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 312ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 525, 350, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_imgs = autoencoder.predict(batch)\n",
    "processed_imgs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proper autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = layers.Tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_encoder(embed_dim=30, image_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)):\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=image_shape),\n",
    "        layers.Conv2D(filters=32, kernel_size=3, strides=2, activation='relu'),\n",
    "        layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(embed_dim),\n",
    "    ])\n",
    "    \n",
    "def conv_decoder(embed_dim, image_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)):\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=(embed_dim,)),\n",
    "        layers.Dense(units=32, activation='relu'),\n",
    "        layers.Reshape(target_shape=(4, 4, 2)),\n",
    "        layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "        layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "        layers.Conv2DTranspose(filters=3, kernel_size=3, strides=1, activation='sigmoid', padding='same')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lew2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
